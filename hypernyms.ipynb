{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "from time import time\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "def try_print(w2v, test_word):\n",
    "    try:\n",
    "        for word, score in w2v.most_similar(test_word):\n",
    "            print(word, score)\n",
    "    except:\n",
    "        print(\"Warning: word '{}' not found.\".format(test_word))\n",
    "        \n",
    "    \n",
    "def load_and_pickle(w2v_fpath, binary=False):\n",
    "    tic = time()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    w2v_pkl_fpath = w2v_fpath + \".pkl\"\n",
    "\n",
    "    if exists(w2v_pkl_fpath):\n",
    "        w2v = KeyedVectors.load(w2v_pkl_fpath)\n",
    "    else:\n",
    "        w2v = KeyedVectors.load_word2vec_format(w2v_fpath, binary=binary, unicode_errors='ignore')\n",
    "        w2v.init_sims(replace=True)\n",
    "        try_print(w2v, \"for\")\n",
    "        try_print(w2v, \"для\")\n",
    "        w2v.save(w2v_pkl_fpath)\n",
    "    \n",
    "    print(time()- tic, \"sec.\")\n",
    "\n",
    "    return w2v, w2v_pkl_fpath\n",
    "\n",
    "w2v_en_original_fpath = \"/home/panchenko/tmp/GoogleNews-vectors-negative300.txt\"\n",
    "w2v_ru_original_fpath = \"/home/panchenko/tmp/all.norm-sz500-w10-cb0-it3-min5.w2v\"\n",
    "\n",
    "# w2v_en, w2v_en_fpath = load_and_pickle(w2v_en_original_fpath)\n",
    "# w2v_ru, w2v_ru_fpath = load_and_pickle(w2v_ru_original_fpath, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-07 17:11:13,304 : INFO : loading EuclideanKeyedVectors object from /home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv-1000-sum-score-20.sense_vectors.pkl\n",
      "2018-06-07 17:11:13,490 : INFO : loading syn0 from /home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv-1000-sum-score-20.sense_vectors.pkl.syn0.npy with mmap=None\n",
      "2018-06-07 17:11:13,568 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-07 17:11:13,569 : INFO : loaded /home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv-1000-sum-score-20.sense_vectors.pkl\n",
      "2018-06-07 17:11:13,576 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2653944492340088 sec.\n",
      "hV45689#0 0.9192017316818237\n",
      "sV45983#0 0.9028427004814148\n",
      "hV43990#0 0.8921349048614502\n",
      "sV48807#0 0.6990944743156433\n",
      "sV48540#0 0.6922570466995239\n",
      "hV47962#0 0.6613180041313171\n",
      "hV43059#0 0.6459588408470154\n",
      "hV49073#0 0.6402440667152405\n",
      "sV43611#0 0.6287655234336853\n",
      "sV46623#0 0.6096590161323547\n"
     ]
    }
   ],
   "source": [
    "s2v, s2v_pkl = load_and_pickle(\"/home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv-1000-sum-score-20.sense_vectors\")\n",
    "\n",
    "try_print(s2v, \"hV49368#0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'вредить#1': 0.501205,\n",
       "         'говорить#1': 0.26525,\n",
       "         'красить#2': 0.386612,\n",
       "         'навредить#1': 0.501205,\n",
       "         'опустить#1': 0.256847,\n",
       "         'ругать#1': 0.253083})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsv.pcz.data[\"hV44013\"][0][\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model...\n",
      "Loaded 72143 words from: /home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a344dd42eece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mweight_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         max_cluster_words=20)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# for max_top_synsets in range(1,10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sensegram/vector_representations/dense_sense_vectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pcz_fpath, word_vectors_obj, save_pkl, sense_dim_num, norm_type, weight_type, max_cluster_words)\u001b[0m\n\u001b[1;32m     12\u001b[0m             norm_type=\"sum\", weight_type=\"score\", max_cluster_words=20):\n\u001b[1;32m     13\u001b[0m         super(DenseSenseVectors, self).__init__(pcz_fpath, word_vectors_obj,\n\u001b[0;32m---> 14\u001b[0;31m             save_pkl, sense_dim_num, norm_type, weight_type, max_cluster_words)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_sense2vector_precomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sensegram/vector_representations/sense_vectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pcz_fpath, word_vectors_obj, save_pkl, sense_dim_num, norm_type, weight_type, max_cluster_words)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcz_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSenseClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcz_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_dst_senses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_sim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msense_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msense_vectors_bin_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msense_vectors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vectors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sensegram/vector_representations/sense_vectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, sense2vector_fpath)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0msense2vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_sense2vector_precomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded a pre-computed model from:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sensegram/vector_representations/dense_sense_vectors.py\u001b[0m in \u001b[0;36m_load_sense2vector_precomp\u001b[0;34m(self, sense2vector_fpath)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_sense2vector_precomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSenseGram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense2vector_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_most_probable_sense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sensegram/sensegram.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, model_fpath, fvocab, binary, norm_only, encoding, unicode_errors)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Load word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mwv_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import operator\n",
    "from multiprocessing import Pool\n",
    "from vector_representations.dense_sense_vectors import DenseSenseVectors\n",
    "from traceback import format_exc\n",
    "\n",
    "\n",
    "def generate_binary_hypers(output_dir, max_synsets=1, hyper_synset_max_size=10, hc_max=0):\n",
    "    output_fpath = output_dir + \"vector-link-s%d-hmx%d-hc%d.csv\" % (\n",
    "        max_synsets, hyper_synset_max_size, hc_max)  \n",
    "    bin_count = 0\n",
    "    \n",
    "    out = codecs.open(output_fpath, \"w\", \"utf-8\")\n",
    "    log = codecs.open(output_fpath + \".log\", \"w\", \"utf-8\")\n",
    "    \n",
    "    for i, h_id in enumerate(dsv.pcz.data):\n",
    "        try:\n",
    "            if i % 10000 == 0: print(i)\n",
    "\n",
    "            if \"h\" in h_id:\n",
    "                hypo_h_senses = dsv.pcz.data[h_id][0][\"cluster\"]\n",
    "                tmp = sorted(dsv.pcz.data[h_id][0][\"cluster\"].items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "                s_id = \"s\" + h_id[1:]\n",
    "                hypo_senses = dsv.pcz.data[s_id][0][\"cluster\"]\n",
    "                log.write(\"\\n{}\\t{}\\n\".format(\n",
    "                    h_id, \", \".join(hypo_h_senses)\n",
    "                ))\n",
    "                log.write(\"{}\\n\".format(\n",
    "                    \", \".join([\"{}:{}\".format(k,v) for k,v in tmp])\n",
    "                ))\n",
    "                log.write(\"{}\\t{}\\n\".format(\n",
    "                    s_id, \", \".join(hypo_senses)\n",
    "                ))\n",
    "\n",
    "                # save relations from the hierarchical context \n",
    "                for hypo_sense in hypo_senses:\n",
    "                    for hc_num, hyper_sense in enumerate(hypo_h_senses):\n",
    "                        if hc_num == hc_max: break\n",
    "                        hypo_word = hypo_sense.split(\"#\")[0]\n",
    "                        hyper_word = hyper_sense.split(\"#\")[0]\n",
    "                        if hypo_word != hyper_word:\n",
    "                            out.write(\"{}\\t{}\\tfrom-original-labels\\n\".format(hypo_word, hyper_word))\n",
    "                    bin_count += 1\n",
    "\n",
    "                # save binary relations from a synset\n",
    "                s_synsets = 0\n",
    "                for rh_id, s in dsv.sense_vectors.most_similar(h_id + \"#0\"):\n",
    "                    if \"s\" in rh_id:\n",
    "                        hyper_senses = dsv.pcz.data[rh_id.split(\"#\")[0]][0][\"cluster\"]\n",
    "                        if len(hyper_senses) > hyper_synset_max_size: continue\n",
    "\n",
    "                        rh_str = \", \".join(hyper_senses)\n",
    "                        log.write(\"\\t{}:{:.3f} {}\\n\".format(rh_id, s, rh_str))\n",
    "\n",
    "                        for hypo_sense in hypo_senses:\n",
    "                            for hyper_sense in hyper_senses:\n",
    "                                hypo_word = hypo_sense.split(\"#\")[0]\n",
    "                                hyper_word = hyper_sense.split(\"#\")[0]\n",
    "                                if hypo_word != hyper_word:\n",
    "                                    out.write(\"{}\\t{}\\tfrom-vector-linkage\\n\".format(hypo_word, hyper_word))\n",
    "                                bin_count += 1\n",
    "                        s_synsets += 1\n",
    "\n",
    "                        if s_synsets >= max_synsets: break\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except:\n",
    "            print(\"Error\", i, h_id)\n",
    "            print(format_exc())\n",
    "    out.close()\n",
    "    log.close()\n",
    "    \n",
    "    print(\"# binary relations:\", bin_count)\n",
    "    print(\"binary relations:\", output_fpath)\n",
    "    print(\"log of binary relations:\", output_fpath + \".log\")\n",
    "    \n",
    "    return (bin_count, output_fpath)\n",
    "    \n",
    "\n",
    "output_dir = \"/home/panchenko/tmp/vector-link/konvens/ru/\"\n",
    "pcz_fpath=\"/home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv\"\n",
    "\n",
    "reload = False\n",
    "try: dsv\n",
    "except NameError: reload = True\n",
    "\n",
    "if reload:\n",
    "    dsv = DenseSenseVectors(\n",
    "        pcz_fpath=pcz_fpath,\n",
    "        word_vectors_obj=None,\n",
    "        save_pkl=True,\n",
    "        sense_dim_num=1000,\n",
    "        norm_type=\"sum\",\n",
    "        weight_type=\"score\",\n",
    "        max_cluster_words=20)\n",
    " \n",
    "# for max_top_synsets in range(1,10):\n",
    "#     for max_hyper_synset_size in [3, 5, 10, 15, 20]:\n",
    "#         for hc_max in [1, 2, 3, 0]: \n",
    "#             p = (output_dir, max_top_synsets, max_hyper_synset_size, hc_max)\n",
    "# with terminating(Pool(32)) as pool:\n",
    "#     for res in pool.imap_unordered(runp, todo):\n",
    "#         print res\n",
    "     \n",
    "for max_top_synsets in range(1,10):\n",
    "    for max_hyper_synset_size in [3, 5, 10, 15, 20]:\n",
    "        for hc_max in [1, 2, 3, 0]: \n",
    "            print(\"=\"*50)\n",
    "            print(\"max number of synsets:\", max_top_synsets)\n",
    "            print(\"max hyper synset size:\", max_hyper_synset_size)\n",
    "            print(\"hc_max:\", hc_max)\n",
    "            generate_binary_hypers(output_dir, max_top_synsets, max_hyper_synset_size, hc_max)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hs_type = \"h\"\n",
    "min_size = 3\n",
    "n = 0\n",
    "for i, hs_id in enumerate(dsv.pcz.data):\n",
    "    synset_len = len(dsv.pcz.data[hs_id][0][\"cluster\"])\n",
    "    if synset_len >= min_size and hs_type in hs_id:\n",
    "        print \"\\n\", hs_id, \", \".join(dsv.pcz.data[hs_id][0][\"cluster\"])\n",
    "        s_id = \"s\" + hs_id[1:]\n",
    "        print s_id, \", \".join(dsv.pcz.data[s_id][0][\"cluster\"])\n",
    "        for rhs_id, s in dsv.sense_vectors.most_similar(hs_id + \"#0\"):\n",
    "            rhs_str = \", \".join(dsv.pcz.data[rhs_id.split(\"#\")[0]][0][\"cluster\"])\n",
    "            print \"\\t%s:%.3f %s\" % (rhs_id, s, rhs_str)\n",
    "        n += 1\n",
    "    if n > 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h1',\n",
       " 'h1#1',\n",
       " 'hA100',\n",
       " 'hA10003',\n",
       " 'hA10005',\n",
       " 'hA10007',\n",
       " 'hA10011',\n",
       " 'hA10014',\n",
       " 'hA10025',\n",
       " 'hA10037',\n",
       " 'hA10047',\n",
       " 'hA10061',\n",
       " 'hA10068',\n",
       " 'hA10071',\n",
       " 'hA10073',\n",
       " 'hA10078',\n",
       " 'hA10079',\n",
       " 'hA10086',\n",
       " 'hA10099',\n",
       " 'hA1010',\n",
       " 'hA10103',\n",
       " 'hA10111',\n",
       " 'hA10113',\n",
       " 'hA10114',\n",
       " 'hA10115',\n",
       " 'hA10117',\n",
       " 'hA10125',\n",
       " 'hA1013',\n",
       " 'hA10130',\n",
       " 'hA10131',\n",
       " 'hA10133',\n",
       " 'hA10134',\n",
       " 'hA10138',\n",
       " 'hA1015',\n",
       " 'hA10152',\n",
       " 'hA10156',\n",
       " 'hA1016',\n",
       " 'hA10162',\n",
       " 'hA10170',\n",
       " 'hA10177',\n",
       " 'hA10178',\n",
       " 'hA10183',\n",
       " 'hA10187',\n",
       " 'hA10195',\n",
       " 'hA10206',\n",
       " 'hA10208',\n",
       " 'hA10210',\n",
       " 'hA10212',\n",
       " 'hA10213',\n",
       " 'hA10215',\n",
       " 'hA10222',\n",
       " 'hA10226',\n",
       " 'hA10232',\n",
       " 'hA10239',\n",
       " 'hA10245',\n",
       " 'hA10246',\n",
       " 'hA10251',\n",
       " 'hA10256',\n",
       " 'hA1026',\n",
       " 'hA10268',\n",
       " 'hA10271',\n",
       " 'hA10272',\n",
       " 'hA10279',\n",
       " 'hA10281',\n",
       " 'hA10283',\n",
       " 'hA10287',\n",
       " 'hA10290',\n",
       " 'hA10291',\n",
       " 'hA10294',\n",
       " 'hA10297',\n",
       " 'hA10298',\n",
       " 'hA10299',\n",
       " 'hA10309',\n",
       " 'hA10311',\n",
       " 'hA10312',\n",
       " 'hA10314',\n",
       " 'hA1032',\n",
       " 'hA10338',\n",
       " 'hA10343',\n",
       " 'hA10344',\n",
       " 'hA10346',\n",
       " 'hA10362',\n",
       " 'hA10364',\n",
       " 'hA10366',\n",
       " 'hA10367',\n",
       " 'hA1038',\n",
       " 'hA10391',\n",
       " 'hA10394',\n",
       " 'hA10399',\n",
       " 'hA10402',\n",
       " 'hA10413',\n",
       " 'hA10414',\n",
       " 'hA10418',\n",
       " 'hA10423',\n",
       " 'hA10425',\n",
       " 'hA10428',\n",
       " 'hA10458',\n",
       " 'hA1046',\n",
       " 'hA10474',\n",
       " 'hA10478',\n",
       " 'hA1048',\n",
       " 'hA10481',\n",
       " 'hA10484',\n",
       " 'hA10485',\n",
       " 'hA10488',\n",
       " 'hA10489',\n",
       " 'hA10494',\n",
       " 'hA105',\n",
       " 'hA10500',\n",
       " 'hA10503',\n",
       " 'hA10504',\n",
       " 'hA10507',\n",
       " 'hA10510',\n",
       " 'hA10511',\n",
       " 'hA10516',\n",
       " 'hA10521',\n",
       " 'hA10533',\n",
       " 'hA10538',\n",
       " 'hA10542',\n",
       " 'hA10548',\n",
       " 'hA10550',\n",
       " 'hA10552',\n",
       " 'hA10554',\n",
       " 'hA10557',\n",
       " 'hA10564',\n",
       " 'hA10568',\n",
       " 'hA10572',\n",
       " 'hA1058',\n",
       " 'hA10582',\n",
       " 'hA10589',\n",
       " 'hA1059',\n",
       " 'hA10594',\n",
       " 'hA10596',\n",
       " 'hA10598',\n",
       " 'hA10599',\n",
       " 'hA106',\n",
       " 'hA10601',\n",
       " 'hA10602',\n",
       " 'hA10603',\n",
       " 'hA10604',\n",
       " 'hA10608',\n",
       " 'hA10610',\n",
       " 'hA10617',\n",
       " 'hA10618',\n",
       " 'hA10625',\n",
       " 'hA10629',\n",
       " 'hA10630',\n",
       " 'hA10646',\n",
       " 'hA10647',\n",
       " 'hA10654',\n",
       " 'hA10655',\n",
       " 'hA10664',\n",
       " 'hA10665',\n",
       " 'hA10681',\n",
       " 'hA10684',\n",
       " 'hA10685',\n",
       " 'hA10687',\n",
       " 'hA1069',\n",
       " 'hA10696',\n",
       " 'hA10699',\n",
       " 'hA10701',\n",
       " 'hA10702',\n",
       " 'hA10705',\n",
       " 'hA10710',\n",
       " 'hA10711',\n",
       " 'hA10716',\n",
       " 'hA10717',\n",
       " 'hA1072',\n",
       " 'hA10722',\n",
       " 'hA10727',\n",
       " 'hA10728',\n",
       " 'hA10729',\n",
       " 'hA1073',\n",
       " 'hA10740',\n",
       " 'hA10743',\n",
       " 'hA10758',\n",
       " 'hA10760',\n",
       " 'hA10777',\n",
       " 'hA1078',\n",
       " 'hA10780',\n",
       " 'hA10792',\n",
       " 'hA10794',\n",
       " 'hA10800',\n",
       " 'hA10801',\n",
       " 'hA10810',\n",
       " 'hA10815',\n",
       " 'hA10824',\n",
       " 'hA1083',\n",
       " 'hA10832',\n",
       " 'hA10835',\n",
       " 'hA10838',\n",
       " 'hA1084',\n",
       " 'hA10848',\n",
       " 'hA10852',\n",
       " 'hA10859',\n",
       " 'hA1086',\n",
       " 'hA10860',\n",
       " 'hA10867',\n",
       " 'hA10868',\n",
       " 'hA1087',\n",
       " 'hA10870',\n",
       " 'hA10877',\n",
       " 'hA10878',\n",
       " 'hA10879',\n",
       " 'hA10882',\n",
       " 'hA10887',\n",
       " 'hA10902',\n",
       " 'hA10903',\n",
       " 'hA10907',\n",
       " 'hA10914',\n",
       " 'hA10916',\n",
       " 'hA10925',\n",
       " 'hA10935',\n",
       " 'hA10936',\n",
       " 'hA1094',\n",
       " 'hA10944',\n",
       " 'hA10953',\n",
       " 'hA10975',\n",
       " 'hA10977',\n",
       " 'hA1098',\n",
       " 'hA10983',\n",
       " 'hA10984',\n",
       " 'hA10985',\n",
       " 'hA10994',\n",
       " 'hA10995',\n",
       " 'hA10996',\n",
       " 'hA1100',\n",
       " 'hA11002',\n",
       " 'hA11008',\n",
       " 'hA1102',\n",
       " 'hA11020',\n",
       " 'hA11025',\n",
       " 'hA11036',\n",
       " 'hA11038',\n",
       " 'hA11040',\n",
       " 'hA11041',\n",
       " 'hA11046',\n",
       " 'hA11049',\n",
       " 'hA11053',\n",
       " 'hA11056',\n",
       " 'hA11064',\n",
       " 'hA11070',\n",
       " 'hA11072',\n",
       " 'hA1108',\n",
       " 'hA11083',\n",
       " 'hA11089',\n",
       " 'hA1109',\n",
       " 'hA11096',\n",
       " 'hA11118',\n",
       " 'hA11123',\n",
       " 'hA11124',\n",
       " 'hA1113',\n",
       " 'hA11142',\n",
       " 'hA11144',\n",
       " 'hA11147',\n",
       " 'hA11155',\n",
       " 'hA11159',\n",
       " 'hA1116',\n",
       " 'hA11161',\n",
       " 'hA11166',\n",
       " 'hA11179',\n",
       " 'hA11183',\n",
       " 'hA11190',\n",
       " 'hA11193',\n",
       " 'hA11198',\n",
       " 'hA11202',\n",
       " 'hA11206',\n",
       " 'hA11219',\n",
       " 'hA1122',\n",
       " 'hA11220',\n",
       " 'hA11221',\n",
       " 'hA11227',\n",
       " 'hA11229',\n",
       " 'hA1123',\n",
       " 'hA11233',\n",
       " 'hA11238',\n",
       " 'hA11239',\n",
       " 'hA11243',\n",
       " 'hA11249',\n",
       " 'hA11266',\n",
       " 'hA11281',\n",
       " 'hA11282',\n",
       " 'hA11286',\n",
       " 'hA11287',\n",
       " 'hA11291',\n",
       " 'hA11294',\n",
       " 'hA11296',\n",
       " 'hA11297',\n",
       " 'hA11302',\n",
       " 'hA11304',\n",
       " 'hA11305',\n",
       " 'hA11314',\n",
       " 'hA11316',\n",
       " 'hA11319',\n",
       " 'hA11331',\n",
       " 'hA11334',\n",
       " 'hA11348',\n",
       " 'hA11349',\n",
       " 'hA11358',\n",
       " 'hA11359',\n",
       " 'hA11364',\n",
       " 'hA11367',\n",
       " 'hA11368',\n",
       " 'hA11371',\n",
       " 'hA11376',\n",
       " 'hA11378',\n",
       " 'hA11385',\n",
       " 'hA11387',\n",
       " 'hA11394',\n",
       " 'hA11395',\n",
       " 'hA11397',\n",
       " 'hA11398',\n",
       " 'hA11408',\n",
       " 'hA1141',\n",
       " 'hA11410',\n",
       " 'hA11429',\n",
       " 'hA1143',\n",
       " 'hA11447',\n",
       " 'hA11451',\n",
       " 'hA11455',\n",
       " 'hA11456',\n",
       " 'hA11461',\n",
       " 'hA11463',\n",
       " 'hA11467',\n",
       " 'hA11485',\n",
       " 'hA11512',\n",
       " 'hA1152',\n",
       " 'hA11521',\n",
       " 'hA11530',\n",
       " 'hA11535',\n",
       " 'hA11545',\n",
       " 'hA11551',\n",
       " 'hA11552',\n",
       " 'hA11560',\n",
       " 'hA11568',\n",
       " 'hA11573',\n",
       " 'hA11575',\n",
       " 'hA11579',\n",
       " 'hA11590',\n",
       " 'hA11596',\n",
       " 'hA11604',\n",
       " 'hA11607',\n",
       " 'hA11619',\n",
       " 'hA11625',\n",
       " 'hA11633',\n",
       " 'hA11643',\n",
       " 'hA11648',\n",
       " 'hA1165',\n",
       " 'hA11655',\n",
       " 'hA11658',\n",
       " 'hA11659',\n",
       " 'hA1166',\n",
       " 'hA11671',\n",
       " 'hA11676',\n",
       " 'hA11678',\n",
       " 'hA1168',\n",
       " 'hA11685',\n",
       " 'hA11687',\n",
       " 'hA11690',\n",
       " 'hA11691',\n",
       " 'hA11694',\n",
       " 'hA11698',\n",
       " 'hA11702',\n",
       " 'hA11709',\n",
       " 'hA1172',\n",
       " 'hA11721',\n",
       " 'hA11725',\n",
       " 'hA11726',\n",
       " 'hA11728',\n",
       " 'hA11729',\n",
       " 'hA1173',\n",
       " 'hA11734',\n",
       " 'hA11737',\n",
       " 'hA11745',\n",
       " 'hA11749',\n",
       " 'hA11760',\n",
       " 'hA11761',\n",
       " 'hA11765',\n",
       " 'hA11767',\n",
       " 'hA11774',\n",
       " 'hA11798',\n",
       " 'hA1180',\n",
       " 'hA11809',\n",
       " 'hA11812',\n",
       " 'hA11813',\n",
       " 'hA11816',\n",
       " 'hA11819',\n",
       " 'hA11822',\n",
       " 'hA11823',\n",
       " 'hA11838',\n",
       " 'hA11849',\n",
       " 'hA11853',\n",
       " 'hA11867',\n",
       " 'hA11872',\n",
       " 'hA11881',\n",
       " 'hA11885',\n",
       " 'hA11886',\n",
       " 'hA11887',\n",
       " 'hA11889',\n",
       " 'hA11893',\n",
       " 'hA11898',\n",
       " 'hA11899',\n",
       " 'hA11907',\n",
       " 'hA11916',\n",
       " 'hA11928',\n",
       " 'hA11936',\n",
       " 'hA11940',\n",
       " 'hA11941',\n",
       " 'hA11955',\n",
       " 'hA11965',\n",
       " 'hA1197',\n",
       " 'hA11975',\n",
       " 'hA11981',\n",
       " 'hA11985',\n",
       " 'hA11991',\n",
       " 'hA11995',\n",
       " 'hA1200',\n",
       " 'hA12000',\n",
       " 'hA12002',\n",
       " 'hA12009',\n",
       " 'hA12016',\n",
       " 'hA1202',\n",
       " 'hA12025',\n",
       " 'hA12031',\n",
       " 'hA12033',\n",
       " 'hA12040',\n",
       " 'hA12043',\n",
       " 'hA12057',\n",
       " 'hA12059',\n",
       " 'hA12069',\n",
       " 'hA12076',\n",
       " 'hA1208',\n",
       " 'hA12080',\n",
       " 'hA12089',\n",
       " 'hA12093',\n",
       " 'hA1210',\n",
       " 'hA12103',\n",
       " 'hA12109',\n",
       " 'hA12113',\n",
       " 'hA12124',\n",
       " 'hA12130',\n",
       " 'hA12131',\n",
       " 'hA12132',\n",
       " 'hA12133',\n",
       " 'hA12134',\n",
       " 'hA12136',\n",
       " 'hA12140',\n",
       " 'hA12144',\n",
       " 'hA12145',\n",
       " 'hA12158',\n",
       " 'hA1216',\n",
       " 'hA12161',\n",
       " 'hA12170',\n",
       " 'hA12182',\n",
       " 'hA12189',\n",
       " 'hA12200',\n",
       " 'hA12201',\n",
       " 'hA12212',\n",
       " 'hA12225',\n",
       " 'hA1223',\n",
       " 'hA12231',\n",
       " 'hA1224',\n",
       " 'hA12249',\n",
       " 'hA12250',\n",
       " 'hA12260',\n",
       " 'hA12261',\n",
       " 'hA12263',\n",
       " 'hA12267',\n",
       " 'hA12273',\n",
       " 'hA12282',\n",
       " 'hA12287',\n",
       " 'hA1229',\n",
       " 'hA12295',\n",
       " 'hA12297',\n",
       " 'hA123',\n",
       " 'hA12303',\n",
       " 'hA12305',\n",
       " 'hA12307',\n",
       " 'hA12308',\n",
       " 'hA12310',\n",
       " 'hA12311',\n",
       " 'hA12312',\n",
       " 'hA12314',\n",
       " 'hA12316',\n",
       " 'hA12326',\n",
       " 'hA12334',\n",
       " 'hA12336',\n",
       " 'hA12337',\n",
       " 'hA12338',\n",
       " 'hA12339',\n",
       " 'hA1234',\n",
       " 'hA12342',\n",
       " 'hA12346',\n",
       " 'hA12349',\n",
       " 'hA1235',\n",
       " 'hA12350',\n",
       " 'hA12360',\n",
       " 'hA12364',\n",
       " 'hA12372',\n",
       " 'hA12374',\n",
       " 'hA12382',\n",
       " 'hA12387',\n",
       " 'hA12390',\n",
       " 'hA12392',\n",
       " 'hA12407',\n",
       " 'hA12414',\n",
       " 'hA12422',\n",
       " 'hA12423',\n",
       " 'hA12426',\n",
       " 'hA12438',\n",
       " 'hA12445',\n",
       " 'hA12446',\n",
       " 'hA12468',\n",
       " 'hA12469',\n",
       " 'hA12479',\n",
       " 'hA12485',\n",
       " 'hA12487',\n",
       " 'hA12497',\n",
       " 'hA12501',\n",
       " 'hA12506',\n",
       " 'hA12510',\n",
       " 'hA12515',\n",
       " 'hA12517',\n",
       " 'hA12533',\n",
       " 'hA12534',\n",
       " 'hA12545',\n",
       " 'hA12548',\n",
       " 'hA12566',\n",
       " 'hA12581',\n",
       " 'hA12582',\n",
       " 'hA12586',\n",
       " 'hA1259',\n",
       " 'hA12599',\n",
       " 'hA126',\n",
       " 'hA12612',\n",
       " 'hA12621',\n",
       " 'hA12627',\n",
       " 'hA12633',\n",
       " 'hA1264',\n",
       " 'hA12640',\n",
       " 'hA12645',\n",
       " 'hA1265',\n",
       " 'hA12651',\n",
       " 'hA12653',\n",
       " 'hA12655',\n",
       " 'hA12657',\n",
       " 'hA1267',\n",
       " 'hA1280',\n",
       " 'hA1284',\n",
       " 'hA1287',\n",
       " 'hA1313',\n",
       " 'hA1319',\n",
       " 'hA1330',\n",
       " 'hA1333',\n",
       " 'hA1341',\n",
       " 'hA1347',\n",
       " 'hA1350',\n",
       " 'hA1351',\n",
       " 'hA1352',\n",
       " 'hA1353',\n",
       " 'hA1377',\n",
       " 'hA1385',\n",
       " 'hA1394',\n",
       " 'hA1398',\n",
       " 'hA1410',\n",
       " 'hA1411',\n",
       " 'hA1412',\n",
       " 'hA1415',\n",
       " 'hA1420',\n",
       " 'hA1424',\n",
       " 'hA1425',\n",
       " 'hA1426',\n",
       " 'hA1427',\n",
       " 'hA1428',\n",
       " 'hA1430',\n",
       " 'hA1443',\n",
       " 'hA1448',\n",
       " 'hA1449',\n",
       " 'hA1457',\n",
       " 'hA1464',\n",
       " 'hA1479',\n",
       " 'hA148',\n",
       " 'hA1480',\n",
       " 'hA1483',\n",
       " 'hA1486',\n",
       " 'hA1489',\n",
       " 'hA149',\n",
       " 'hA1490',\n",
       " 'hA1491',\n",
       " 'hA1493',\n",
       " 'hA1496',\n",
       " 'hA152',\n",
       " 'hA1520',\n",
       " 'hA1527',\n",
       " 'hA153',\n",
       " 'hA1532',\n",
       " 'hA1534',\n",
       " 'hA1536',\n",
       " 'hA1542',\n",
       " 'hA1543',\n",
       " 'hA1549',\n",
       " 'hA1563',\n",
       " 'hA1564',\n",
       " 'hA157',\n",
       " 'hA1572',\n",
       " 'hA1576',\n",
       " 'hA1591',\n",
       " 'hA1592',\n",
       " 'hA1597',\n",
       " 'hA1605',\n",
       " 'hA1608',\n",
       " 'hA1615',\n",
       " 'hA1618',\n",
       " 'hA1619',\n",
       " 'hA1621',\n",
       " 'hA1622',\n",
       " 'hA163',\n",
       " 'hA1633',\n",
       " 'hA1639',\n",
       " 'hA164',\n",
       " 'hA1642',\n",
       " 'hA1643',\n",
       " 'hA1650',\n",
       " 'hA1665',\n",
       " 'hA1670',\n",
       " 'hA1677',\n",
       " 'hA1678',\n",
       " 'hA1681',\n",
       " 'hA1684',\n",
       " 'hA1691',\n",
       " 'hA1693',\n",
       " 'hA1695',\n",
       " 'hA1696',\n",
       " 'hA1703',\n",
       " 'hA171',\n",
       " 'hA1717',\n",
       " 'hA1723',\n",
       " 'hA1726',\n",
       " 'hA1730',\n",
       " 'hA1734',\n",
       " 'hA1737',\n",
       " 'hA1744',\n",
       " 'hA1746',\n",
       " 'hA1749',\n",
       " 'hA1756',\n",
       " 'hA1767',\n",
       " 'hA177',\n",
       " 'hA1777',\n",
       " 'hA1780',\n",
       " 'hA1783',\n",
       " 'hA1787',\n",
       " 'hA1793',\n",
       " 'hA180',\n",
       " 'hA1800',\n",
       " 'hA1801',\n",
       " 'hA1807',\n",
       " 'hA1817',\n",
       " 'hA1822',\n",
       " 'hA1830',\n",
       " 'hA1839',\n",
       " 'hA1841',\n",
       " 'hA1845',\n",
       " 'hA1851',\n",
       " 'hA1874',\n",
       " 'hA1880',\n",
       " 'hA1885',\n",
       " 'hA1888',\n",
       " 'hA1892',\n",
       " 'hA1908',\n",
       " 'hA1909',\n",
       " 'hA1911',\n",
       " 'hA1919',\n",
       " 'hA192',\n",
       " 'hA193',\n",
       " 'hA1931',\n",
       " 'hA1936',\n",
       " 'hA1943',\n",
       " 'hA1947',\n",
       " 'hA1948',\n",
       " 'hA1960',\n",
       " 'hA1962',\n",
       " 'hA1972',\n",
       " 'hA1975',\n",
       " 'hA1985',\n",
       " 'hA199',\n",
       " 'hA2003',\n",
       " 'hA2006',\n",
       " 'hA202',\n",
       " 'hA2027',\n",
       " 'hA2029',\n",
       " 'hA2030',\n",
       " 'hA2036',\n",
       " 'hA2045',\n",
       " 'hA205',\n",
       " 'hA2052',\n",
       " 'hA2053',\n",
       " 'hA2066',\n",
       " 'hA2069',\n",
       " 'hA208',\n",
       " 'hA2101',\n",
       " 'hA2113',\n",
       " 'hA2119',\n",
       " 'hA2120',\n",
       " 'hA2121',\n",
       " 'hA2122',\n",
       " 'hA2125',\n",
       " 'hA2127',\n",
       " 'hA213',\n",
       " 'hA2133',\n",
       " 'hA2137',\n",
       " 'hA2141',\n",
       " 'hA2143',\n",
       " 'hA2154',\n",
       " 'hA2155',\n",
       " 'hA2170',\n",
       " 'hA2171',\n",
       " 'hA2175',\n",
       " 'hA2189',\n",
       " 'hA2195',\n",
       " 'hA22',\n",
       " 'hA2202',\n",
       " 'hA2211',\n",
       " 'hA2213',\n",
       " 'hA2217',\n",
       " 'hA2220',\n",
       " 'hA2227',\n",
       " 'hA223',\n",
       " 'hA2240',\n",
       " 'hA2246',\n",
       " 'hA2248',\n",
       " 'hA225',\n",
       " 'hA2253',\n",
       " 'hA2257',\n",
       " 'hA2258',\n",
       " 'hA2265',\n",
       " 'hA2269',\n",
       " 'hA227',\n",
       " 'hA2273',\n",
       " 'hA2276',\n",
       " 'hA228',\n",
       " 'hA2297',\n",
       " 'hA2298',\n",
       " 'hA2303',\n",
       " 'hA2306',\n",
       " 'hA231',\n",
       " 'hA2311',\n",
       " 'hA2316',\n",
       " 'hA2322',\n",
       " 'hA2325',\n",
       " 'hA2327',\n",
       " 'hA2330',\n",
       " 'hA2337',\n",
       " 'hA2346',\n",
       " 'hA2349',\n",
       " 'hA2352',\n",
       " 'hA2355',\n",
       " 'hA2357',\n",
       " 'hA2362',\n",
       " 'hA2370',\n",
       " 'hA2371',\n",
       " 'hA2373',\n",
       " 'hA2382',\n",
       " 'hA2396',\n",
       " 'hA2399',\n",
       " 'hA2401',\n",
       " 'hA2409',\n",
       " 'hA2417',\n",
       " 'hA2431',\n",
       " 'hA2474',\n",
       " 'hA2478',\n",
       " 'hA248',\n",
       " 'hA2486',\n",
       " 'hA2489',\n",
       " 'hA2490',\n",
       " 'hA2493',\n",
       " 'hA2498',\n",
       " 'hA2501',\n",
       " 'hA2504',\n",
       " 'hA2514',\n",
       " 'hA2518',\n",
       " 'hA2533',\n",
       " 'hA2534',\n",
       " 'hA2540',\n",
       " 'hA2546',\n",
       " 'hA2547',\n",
       " 'hA2549',\n",
       " 'hA2552',\n",
       " 'hA2554',\n",
       " 'hA256',\n",
       " 'hA2562',\n",
       " 'hA2582',\n",
       " 'hA2590',\n",
       " 'hA2596',\n",
       " 'hA2608',\n",
       " 'hA2609',\n",
       " 'hA2613',\n",
       " 'hA2617',\n",
       " 'hA2618',\n",
       " 'hA2619',\n",
       " 'hA2635',\n",
       " 'hA2636',\n",
       " 'hA264',\n",
       " 'hA2641',\n",
       " 'hA2645',\n",
       " 'hA2647',\n",
       " 'hA2649',\n",
       " 'hA2651',\n",
       " 'hA2655',\n",
       " 'hA2656',\n",
       " 'hA2666',\n",
       " 'hA2667',\n",
       " 'hA2676',\n",
       " 'hA2686',\n",
       " 'hA2688',\n",
       " 'hA2691',\n",
       " 'hA2697',\n",
       " 'hA2710',\n",
       " 'hA2715',\n",
       " 'hA2721',\n",
       " 'hA2727',\n",
       " 'hA2730',\n",
       " 'hA2732',\n",
       " 'hA2738',\n",
       " 'hA2739',\n",
       " 'hA2742',\n",
       " 'hA275',\n",
       " 'hA2762',\n",
       " 'hA2769',\n",
       " 'hA2776',\n",
       " 'hA2777',\n",
       " 'hA2780',\n",
       " 'hA2794',\n",
       " 'hA2796',\n",
       " 'hA2803',\n",
       " 'hA2808',\n",
       " 'hA281',\n",
       " 'hA2814',\n",
       " 'hA2820',\n",
       " 'hA2831',\n",
       " 'hA2833',\n",
       " 'hA2836',\n",
       " 'hA2843',\n",
       " 'hA2849',\n",
       " 'hA2854',\n",
       " 'hA2858',\n",
       " 'hA2861',\n",
       " 'hA2864',\n",
       " 'hA2870',\n",
       " 'hA2875',\n",
       " 'hA2878',\n",
       " 'hA2879',\n",
       " 'hA2884',\n",
       " 'hA2885',\n",
       " 'hA2886',\n",
       " 'hA2887',\n",
       " 'hA2903',\n",
       " 'hA2913',\n",
       " 'hA2921',\n",
       " 'hA2926',\n",
       " 'hA2928',\n",
       " 'hA2929',\n",
       " 'hA2934',\n",
       " 'hA2935',\n",
       " 'hA2938',\n",
       " 'hA2941',\n",
       " 'hA2968',\n",
       " 'hA2971',\n",
       " 'hA2975',\n",
       " 'hA2980',\n",
       " 'hA2988',\n",
       " 'hA2989',\n",
       " 'hA2994',\n",
       " 'hA2995',\n",
       " 'hA2999',\n",
       " 'hA300',\n",
       " 'hA3008',\n",
       " 'hA3014',\n",
       " 'hA3021',\n",
       " 'hA3022',\n",
       " 'hA3025',\n",
       " 'hA3028',\n",
       " 'hA3031',\n",
       " 'hA3036',\n",
       " 'hA3044',\n",
       " 'hA3047',\n",
       " 'hA3054',\n",
       " 'hA3055',\n",
       " 'hA3057',\n",
       " 'hA3064',\n",
       " 'hA3071',\n",
       " 'hA3074',\n",
       " 'hA309',\n",
       " 'hA3093',\n",
       " 'hA3098',\n",
       " 'hA31',\n",
       " 'hA3105',\n",
       " 'hA3107',\n",
       " 'hA311',\n",
       " 'hA3134',\n",
       " 'hA3137',\n",
       " 'hA3138',\n",
       " 'hA3139',\n",
       " 'hA3144',\n",
       " 'hA3145',\n",
       " 'hA3146',\n",
       " 'hA3149',\n",
       " 'hA3150',\n",
       " 'hA3152',\n",
       " 'hA3167',\n",
       " 'hA317',\n",
       " 'hA3172',\n",
       " 'hA3174',\n",
       " 'hA3188',\n",
       " 'hA3189',\n",
       " 'hA3193',\n",
       " 'hA3201',\n",
       " 'hA3206',\n",
       " 'hA3207',\n",
       " 'hA3208',\n",
       " 'hA3215',\n",
       " 'hA3217',\n",
       " 'hA3218',\n",
       " 'hA3226',\n",
       " 'hA3260',\n",
       " 'hA3269',\n",
       " 'hA327',\n",
       " 'hA3273',\n",
       " 'hA3281',\n",
       " 'hA3285',\n",
       " 'hA3295',\n",
       " 'hA3299',\n",
       " 'hA3300',\n",
       " 'hA3307',\n",
       " 'hA3308',\n",
       " 'hA3313',\n",
       " 'hA3323',\n",
       " 'hA3328',\n",
       " 'hA3329',\n",
       " 'hA3330',\n",
       " 'hA334',\n",
       " 'hA3343',\n",
       " 'hA3352',\n",
       " 'hA3355',\n",
       " 'hA3356',\n",
       " 'hA3361',\n",
       " 'hA3367',\n",
       " 'hA3369',\n",
       " 'hA3375',\n",
       " 'hA3376',\n",
       " 'hA3387',\n",
       " 'hA3390',\n",
       " 'hA3405',\n",
       " 'hA3408',\n",
       " 'hA3410',\n",
       " 'hA3414',\n",
       " 'hA3428',\n",
       " 'hA3429',\n",
       " 'hA3431',\n",
       " 'hA3436',\n",
       " 'hA3443',\n",
       " 'hA3447',\n",
       " 'hA3448',\n",
       " 'hA3453',\n",
       " 'hA3460',\n",
       " 'hA3463',\n",
       " 'hA3467',\n",
       " 'hA347',\n",
       " 'hA3471',\n",
       " 'hA3473',\n",
       " 'hA3475',\n",
       " 'hA3479',\n",
       " 'hA3482',\n",
       " 'hA3485',\n",
       " 'hA3489',\n",
       " 'hA3500',\n",
       " 'hA3503',\n",
       " 'hA3508',\n",
       " 'hA3512',\n",
       " 'hA3515',\n",
       " 'hA3518',\n",
       " 'hA3524',\n",
       " 'hA353',\n",
       " 'hA3539',\n",
       " 'hA3540',\n",
       " 'hA3542',\n",
       " 'hA355',\n",
       " 'hA3555',\n",
       " 'hA3560',\n",
       " 'hA3563',\n",
       " 'hA3565',\n",
       " 'hA3567',\n",
       " 'hA3583',\n",
       " 'hA359',\n",
       " 'hA3590',\n",
       " 'hA3591',\n",
       " 'hA3595',\n",
       " 'hA3600',\n",
       " 'hA3602',\n",
       " 'hA3603',\n",
       " 'hA3610',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(k for k in dsv.pcz.data.keys() if \"h\" in k)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-07 18:26:33,085 : INFO : loading EuclideanKeyedVectors object from /home/panchenko/tmp/all.norm-sz500-w10-cb0-it3-min5.w2v.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input PCZ: /home/panchenko/tmp/vector-link/konvens/ru/rwn-joint-tfidf-sensegram.tsv\n",
      "Input word vectors: /home/panchenko/tmp/all.norm-sz500-w10-cb0-it3-min5.w2v\n",
      "Sparse: False\n",
      "Type of vector normalization: sum\n",
      "Weight type: score\n",
      "Max. number of cluster words to use: 20\n",
      "Sense dim. number (sparse only): 1000\n",
      "Save pickle (sparse only): False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-07 18:26:49,651 : INFO : loading syn0 from /home/panchenko/tmp/all.norm-sz500-w10-cb0-it3-min5.w2v.pkl.syn0.npy with mmap=None\n",
      "2018-06-07 18:26:56,527 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-07 18:26:56,529 : INFO : loaded /home/panchenko/tmp/all.norm-sz500-w10-cb0-it3-min5.w2v.pkl\n",
      "2018-06-07 18:26:56,530 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "from glob import glob \n",
    "from vector_representations.build_sense_vectors import run\n",
    "\n",
    "for lang in [\"ru\", \"en\"]:\n",
    "    sensegram_fpaths = \"/home/panchenko/tmp/vector-link/konvens/{}/*-sensegram.tsv\".format(lang)\n",
    "    w2v_fpath = w2v_ru_original_fpath if \"ru\" else w2v_en_original_fpath \n",
    "\n",
    "    for inventory_fpath in glob(sensegram_fpaths):\n",
    "\n",
    "        run(inventory_fpath, w2v_fpath) \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
