{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-11 13:00:07,419 : INFO : loading projection weights from model/cc.de.300.vec.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors from: model/cc.de.300.vec.gz\n"
     ]
    }
   ],
   "source": [
    "from word_sense_induction import minimize \n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters\n",
    "from networkx import Graph\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from glob import glob \n",
    "from collections import Counter\n",
    "import codecs\n",
    "from traceback import format_exc\n",
    "import gzip\n",
    "import logging\n",
    "import gensim\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "TOPN = 50\n",
    "verbose = True\n",
    "\n",
    "try:\n",
    "    wv\n",
    "except NameError:\n",
    "    wv = None\n",
    "\n",
    "\n",
    "def get_sorted_vocabulary(vectors_fpath):\n",
    "    with gzip.open(vectors_fpath, \"rb\") as in_f:\n",
    "        vocabulary = []\n",
    "        for i, line in enumerate(in_f):\n",
    "            if i == 0: continue\n",
    "            vocabulary.append( str(line, \"utf-8\").split(\" \")[0] )\n",
    "\n",
    "            \n",
    "def save_to_gensim_format(wv, output_fpath):\n",
    "    tic = time()\n",
    "    wv.save(output_fpath)\n",
    "    print(\"Saved in {} sec.\".format(time()-tic))\n",
    "    \n",
    "\n",
    "def load_globally(word_vectors_fpath):\n",
    "    global wv\n",
    "    \n",
    "    if not wv:\n",
    "        print(\"Loading word vectors from:\", word_vectors_fpath)\n",
    "        tic = time()\n",
    "        if word_vectors_fpath.endswith(\".vec.gz\"):\n",
    "            wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "        else:\n",
    "            wv = gensim.models.KeyedVectors.load(word_vectors_fpath)            \n",
    "        print(\"Loaded in {} sec.\".format(time()-tic))\n",
    "    else:\n",
    "        print(\"Using loaded word vectors.\")\n",
    "\n",
    "    return wv\n",
    "\n",
    "\n",
    "def get_nns(target, topn=TOPN):\n",
    "    nns = wv.most_similar(positive=[target], negative=[], topn=topn)\n",
    "    nns = [(word, score) for word, score in nns if minimize(word) != minimize(target)]\n",
    "    return nns\n",
    "\n",
    "\n",
    "def in_nns(nns, word):\n",
    "    for w, s in nns:\n",
    "        if minimize(word) == minimize(w):\n",
    "            return True\n",
    "        \n",
    "    return False \n",
    "\n",
    "\n",
    "def get_pair(first, second):\n",
    "    pair_lst = sorted([first, second])\n",
    "    sorted_pair = (pair_lst[0], pair_lst[1])\n",
    "    return sorted_pair         \n",
    "\n",
    "\n",
    "def get_disc_pairs(ego, topn=TOPN):  \n",
    "    pairs = set()\n",
    "    nns = get_nns(ego, topn)\n",
    "    \n",
    "    for i in range(len(nns)):\n",
    "        topi = nns[i][0]\n",
    "        nns_topi = get_nns(topi, topn) \n",
    "        nns_untopi = wv.most_similar(positive=[ego], negative=[topi], topn=topn)\n",
    "        untopi = nns_untopi[0][0]\n",
    "        if in_nns(nns, untopi): pairs.add(get_pair(topi, untopi))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_nodes(pairs):\n",
    "    nodes = Counter()\n",
    "    for src, dst in pairs:\n",
    "        nodes.update([src])\n",
    "        nodes.update([dst])\n",
    "        \n",
    "    return nodes\n",
    "\n",
    "\n",
    "def list2dict(lst):\n",
    "    return {p[0]: p[1] for p in lst}\n",
    "\n",
    "\n",
    "def wsi(ego, topn=TOPN):\n",
    "    tic = time()\n",
    "    ego_network = Graph(name=ego)\n",
    "\n",
    "    pairs = get_disc_pairs(ego, topn)\n",
    "    nodes = get_nodes(pairs)   \n",
    "    \n",
    "    ego_network.add_nodes_from( [(node, {'size': size}) for node, size in nodes.items()] )\n",
    "    \n",
    "    for r_node in ego_network:\n",
    "        related_related_nodes = list2dict(get_nns(r_node))\n",
    "        related_related_nodes_ego = sorted(\n",
    "            [(related_related_nodes[rr_node], rr_node) for rr_node in related_related_nodes if rr_node in ego_network],\n",
    "            reverse=True)[:topn]\n",
    "        \n",
    "        related_edges = []\n",
    "        for w, rr_node in related_related_nodes_ego:\n",
    "            if get_pair(r_node, rr_node) not in pairs:\n",
    "                related_edges.append( (r_node, rr_node, {\"weight\": w}) )\n",
    "            else:\n",
    "                print(\"Skipping:\", r_node, rr_node)\n",
    "        ego_network.add_edges_from(related_edges)\n",
    "\n",
    "    chinese_whispers(ego_network, weighting=\"top\", iterations=20)\n",
    "    if verbose: print(\"{}\\t{:f} sec.\".format(ego, time()-tic))\n",
    "\n",
    "    return {\"network\": ego_network,  \"nodes\": nodes}\n",
    "\n",
    "\n",
    "def draw_ego(G, show=False, save_fpath=\"\"):\n",
    "    colors = [1. / G.node[node]['label'] for node in G.nodes()]\n",
    "    sizes = [300. * G.node[node]['size'] for node in G.nodes()]  \n",
    "\n",
    "    plt.clf()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "\n",
    "    nx.draw_networkx(G, cmap=plt.get_cmap('gist_rainbow'),\n",
    "                     node_color=colors,\n",
    "                     font_color='black',\n",
    "                     node_size=sizes)\n",
    "\n",
    "    if show: plt.show()\n",
    "    if save_fpath != \"\":\n",
    "        plt.savefig(save_fpath)\n",
    "        \n",
    "    fig.clf()\n",
    "        \n",
    "\n",
    "def get_target_words():\n",
    "    words = set([\"Maus\", \"Schlange\"])\n",
    "\n",
    "    for pairs_fpath in glob(\"/home/panchenko/eval/de*dataset\"):\n",
    "        df = read_csv(pairs_fpath, sep=\";\", encoding=\"utf-8\")\n",
    "        for i, row in df.iterrows():\n",
    "            words.add(row.word1)\n",
    "            words.add(row.word2)\n",
    "\n",
    "    words = sorted(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_cluster_lines(G, nodes):\n",
    "    lines = []\n",
    "    labels_clusters = sorted(aggregate_clusters(G).items(), key=lambda e: len(e[1]), reverse=True)\n",
    "    for label, cluster in labels_clusters:\n",
    "        scored_words = []\n",
    "        for word in cluster:\n",
    "            scored_words.append( (nodes[word], word) )\n",
    "        keyword = sorted(scored_words, reverse=True)[0][1]\n",
    "        \n",
    "        lines.append(\"{}\\t{}\\t{}\\t{}\\n\".format(G.name, label, keyword, \", \".join(cluster)))\n",
    "        \n",
    "    return lines \n",
    "\n",
    "\n",
    "visualize = False\n",
    "show_plot = False\n",
    "eval_vocabulary = False\n",
    "wv_fpath = \"model/cc.de.300.vec.gz\"\n",
    "wv_gensim_fpath = wv_fpath + \".gensim\"\n",
    "\n",
    "if eval_vocabulary:\n",
    "    words = {w: None for w in get_target_words()}\n",
    "else:\n",
    "    words = get_sorted_vocabulary(wv_fpath)\n",
    "\n",
    "if not exists(wv_gensim_fpath):\n",
    "    load_globally(wv_fpath)\n",
    "    save_to_gensim_format(vw, wv_gensim_fpath)\n",
    "else:\n",
    "    load_globally(wv_gensim_fpath)\n",
    "    \n",
    "for topn in [50, 100, 200]: \n",
    "    output_fpath = wv_fpath + \".top{}.inventory.tsv\".format(topn)\n",
    "    with codecs.open(output_fpath, \"w\", \"utf-8\") as out:\n",
    "        out.write(\"word\\tcid\\tkeyword\\tcluster\\n\")\n",
    "        for word in words:\n",
    "            try:\n",
    "                words[word] = wsi(word, topn=topn)\n",
    "                if visualize:\n",
    "                    plt_fpath = output_fpath + \".{}.png\".format(word)\n",
    "                    draw_ego(words[word][\"network\"], show_plot, plt_fpath)\n",
    "                lines = get_cluster_lines(words[word][\"network\"], words[word][\"nodes\"])\n",
    "                for l in lines: out.write(l)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except:\n",
    "                print(\"Error:\", word)\n",
    "                print(format_exc())\n",
    "    print(\"Output:\", output_fpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "vocabulary = get_sorted_vocabulary(\"model/cc.de.300.vec.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"klasf.vw\"\n",
    "s.endswith(\".vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
