{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-11 12:50:38,621 : INFO : loading Word2VecKeyedVectors object from model/cc.de.300.word_vectors.gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors from: model/cc.de.300.word_vectors.gensim\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'EuclideanKeyedVectors' on <module 'gensim.models.keyedvectors' from '/home/panchenko/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bebe1b6a963a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_target_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mload_globally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bebe1b6a963a>\u001b[0m in \u001b[0;36mload_globally\u001b[0;34m(word_vectors_fpath)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded in {} sec.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'EuclideanKeyedVectors' on <module 'gensim.models.keyedvectors' from '/home/panchenko/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py'>"
     ]
    }
   ],
   "source": [
    "from word_sense_induction import minimize \n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters\n",
    "from networkx import Graph\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from glob import glob \n",
    "from collections import Counter\n",
    "import codecs\n",
    "from traceback import format_exc\n",
    "import gzip\n",
    "import logging\n",
    "import gensim\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "TOPN = 50\n",
    "verbose = True\n",
    "\n",
    "try:\n",
    "    wv\n",
    "except NameError:\n",
    "    wv = None\n",
    "\n",
    "\n",
    "def get_sorted_vocabulary(vectors_fpath):\n",
    "    with gzip.open(vectors_fpath, \"rb\") as in_f:\n",
    "        vocabulary = []\n",
    "        for i, line in enumerate(in_f):\n",
    "            if i == 0: continue\n",
    "            vocabulary.append( str(line, \"utf-8\").split(\" \")[0] )\n",
    "\n",
    "            \n",
    "def save_to_gensim_format(wv, output_fpath):\n",
    "    tic = time()\n",
    "    wv.save(output_fpath)\n",
    "    print(\"Saved in {} sec.\".format(time()-tic))\n",
    "    \n",
    "\n",
    "def load_globally(word_vectors_fpath):\n",
    "    global wv\n",
    "    \n",
    "    if not wv:\n",
    "        print(\"Loading word vectors from:\", word_vectors_fpath)\n",
    "        tic = time()\n",
    "        if word_vectors_fpath.endswith(\".vec.gz\"):\n",
    "            wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "        else:\n",
    "            wv = gensim.models.KeyedVectors.load(word_vectors_fpath)            \n",
    "        print(\"Loaded in {} sec.\".format(time()-tic))\n",
    "    else:\n",
    "        print(\"Using loaded word vectors.\")\n",
    "\n",
    "    return wv\n",
    "\n",
    "\n",
    "def get_nns(target, topn=TOPN):\n",
    "    nns = wv.most_similar(positive=[target], negative=[], topn=topn)\n",
    "    nns = [(word, score) for word, score in nns if minimize(word) != minimize(target)]\n",
    "    return nns\n",
    "\n",
    "\n",
    "def in_nns(nns, word):\n",
    "    for w, s in nns:\n",
    "        if minimize(word) == minimize(w):\n",
    "            return True\n",
    "        \n",
    "    return False \n",
    "\n",
    "\n",
    "def get_pair(first, second):\n",
    "    pair_lst = sorted([first, second])\n",
    "    sorted_pair = (pair_lst[0], pair_lst[1])\n",
    "    return sorted_pair         \n",
    "\n",
    "\n",
    "def get_disc_pairs(ego, topn=TOPN):  \n",
    "    pairs = set()\n",
    "    nns = get_nns(ego, topn)\n",
    "    \n",
    "    for i in range(len(nns)):\n",
    "        topi = nns[i][0]\n",
    "        nns_topi = get_nns(topi, topn) \n",
    "        nns_untopi = wv.most_similar(positive=[ego], negative=[topi], topn=topn)\n",
    "        untopi = nns_untopi[0][0]\n",
    "        if in_nns(nns, untopi): pairs.add(get_pair(topi, untopi))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_nodes(pairs):\n",
    "    nodes = Counter()\n",
    "    for src, dst in pairs:\n",
    "        nodes.update([src])\n",
    "        nodes.update([dst])\n",
    "        \n",
    "    return nodes\n",
    "\n",
    "\n",
    "def list2dict(lst):\n",
    "    return {p[0]: p[1] for p in lst}\n",
    "\n",
    "\n",
    "def wsi(ego, topn=TOPN):\n",
    "    tic = time()\n",
    "    ego_network = Graph(name=ego)\n",
    "\n",
    "    pairs = get_disc_pairs(ego, topn)\n",
    "    nodes = get_nodes(pairs)   \n",
    "    \n",
    "    ego_network.add_nodes_from( [(node, {'size': size}) for node, size in nodes.items()] )\n",
    "    \n",
    "    for r_node in ego_network:\n",
    "        related_related_nodes = list2dict(get_nns(r_node))\n",
    "        related_related_nodes_ego = sorted(\n",
    "            [(related_related_nodes[rr_node], rr_node) for rr_node in related_related_nodes if rr_node in ego_network],\n",
    "            reverse=True)[:topn]\n",
    "        \n",
    "        related_edges = []\n",
    "        for w, rr_node in related_related_nodes_ego:\n",
    "            if get_pair(r_node, rr_node) not in pairs:\n",
    "                related_edges.append( (r_node, rr_node, {\"weight\": w}) )\n",
    "            else:\n",
    "                print(\"Skipping:\", r_node, rr_node)\n",
    "        ego_network.add_edges_from(related_edges)\n",
    "\n",
    "    chinese_whispers(ego_network, weighting=\"top\", iterations=20)\n",
    "    if verbose: print(\"{}\\t{:f} sec.\".format(ego, time()-tic))\n",
    "\n",
    "    return {\"network\": ego_network,  \"nodes\": nodes}\n",
    "\n",
    "\n",
    "def draw_ego(G, show=False, save_fpath=\"\"):\n",
    "    colors = [1. / G.node[node]['label'] for node in G.nodes()]\n",
    "    sizes = [300. * G.node[node]['size'] for node in G.nodes()]  \n",
    "\n",
    "    plt.clf()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "\n",
    "    nx.draw_networkx(G, cmap=plt.get_cmap('gist_rainbow'),\n",
    "                     node_color=colors,\n",
    "                     font_color='black',\n",
    "                     node_size=sizes)\n",
    "\n",
    "    if show: plt.show()\n",
    "    if save_fpath != \"\":\n",
    "        plt.savefig(save_fpath)\n",
    "        \n",
    "    fig.clf()\n",
    "        \n",
    "\n",
    "def get_target_words():\n",
    "    words = set([\"Maus\", \"Schlange\"])\n",
    "\n",
    "    for pairs_fpath in glob(\"/home/panchenko/eval/de*dataset\"):\n",
    "        df = read_csv(pairs_fpath, sep=\";\", encoding=\"utf-8\")\n",
    "        for i, row in df.iterrows():\n",
    "            words.add(row.word1)\n",
    "            words.add(row.word2)\n",
    "\n",
    "    words = sorted(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_cluster_lines(G, nodes):\n",
    "    lines = []\n",
    "    labels_clusters = sorted(aggregate_clusters(G).items(), key=lambda e: len(e[1]), reverse=True)\n",
    "    for label, cluster in labels_clusters:\n",
    "        scored_words = []\n",
    "        for word in cluster:\n",
    "            scored_words.append( (nodes[word], word) )\n",
    "        keyword = sorted(scored_words, reverse=True)[0][1]\n",
    "        \n",
    "        lines.append(\"{}\\t{}\\t{}\\t{}\\n\".format(G.name, label, keyword, \", \".join(cluster)))\n",
    "        \n",
    "    return lines \n",
    "\n",
    "\n",
    "visualize = False\n",
    "show_plot = False\n",
    "eval_vocabulary = False\n",
    "wv_fpath = \"model/cc.de.300.vec.gz\"\n",
    "wv_gensim_fpath + \".gensim\"\n",
    "\n",
    "if eval_vocabulary:\n",
    "    words = {w: None for w in get_target_words()}\n",
    "else:\n",
    "    words = get_sorted_vocabulary(wv_fpath)\n",
    "\n",
    "if not exists(wv_gensim_fpath):\n",
    "    load_globally(wv_fpath)\n",
    "    save_to_gensim_format(vw, wv_gensim_fpath)\n",
    "else:\n",
    "    load_globally(wv_gensim_fpath)\n",
    "    \n",
    "for topn in [50, 100, 200]: \n",
    "    output_fpath = wv_fpath + \".top{}.inventory.tsv\".format(topn)\n",
    "    with codecs.open(output_fpath, \"w\", \"utf-8\") as out:\n",
    "        out.write(\"word\\tcid\\tkeyword\\tcluster\\n\")\n",
    "        for word in words:\n",
    "            try:\n",
    "                words[word] = wsi(word, topn=topn)\n",
    "                if visualize:\n",
    "                    plt_fpath = output_fpath + \".{}.png\".format(word)\n",
    "                    draw_ego(words[word][\"network\"], show_plot, plt_fpath)\n",
    "                lines = get_cluster_lines(words[word][\"network\"], words[word][\"nodes\"])\n",
    "                for l in lines: out.write(l)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except:\n",
    "                print(\"Error:\", word)\n",
    "                print(format_exc())\n",
    "    print(\"Output:\", output_fpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "vocabulary = get_sorted_vocabulary(\"model/cc.de.300.vec.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"klasf.vw\"\n",
    "s.endswith(\".vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
