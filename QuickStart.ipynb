{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating various stages of word sense disambiguation\n",
    "\n",
    "The example below relies on a model for the German language. Usage of the toolkit for other languages is the same.\n",
    "You just need to download a model for the corresponding language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading pre-trained sense vectors \n",
    "\n",
    "To test with word sense embeddings you can use a pretrained model (sense vectors and sense probabilities). These sense         vectors were induced from Wikipedia using word2vec similarities between words in ego-networks. Sense probabilities are       stored in a separate file which is located next to the file with sense vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sensegram\n",
    "\n",
    "# see README for model download information\n",
    "sense_vectors_fpath = \"model/dewiki.txt.clusters.minsize5-1000-sum-score-20.sense_vectors\"\n",
    "sv = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the list of senses of a word \n",
    "\n",
    "Probabilities of senses will be loaded automatically if placed in the same folder as sense vectors and named according to the  same scheme as our pretrained files. To examine how many senses were learned for a word call `get_senses` funcion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hund#1', 0.976526), ('Hund#2', 0.023474)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Hund\"\n",
    "sv.get_senses(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sense aware nearest neighbors\n",
    "\n",
    "The function returns a list of sense names with probabilities for each sense. As one can see, our model has learned two senses for the word \"ключ\".\n",
    "\n",
    "To understand which word sense is represented with a sense vector use `most_similar` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hund#1\n",
      "====================\n",
      "Papagei#1 0.948971\n",
      "Pudel#1 0.948960\n",
      "Mops#1 0.943027\n",
      "Dackel#1 0.938603\n",
      "Teddybär#1 0.934012\n",
      "Zwilling#1 0.930496\n",
      "Kater#1 0.929611\n",
      "Kanarienvogel#1 0.927734\n",
      "Bernhardiner#1 0.926941\n",
      "Kauz#1 0.922378\n",
      "\n",
      "\n",
      "Hund#2\n",
      "====================\n",
      "guter_Kamerad#1 0.942419\n",
      "bunter_Abend#1 0.939721\n",
      "Petschorin#1 0.938157\n",
      "hinzukommender#1 0.937122\n",
      "geplantes_Comeback#1 0.936010\n",
      "schicksalhafter#1 0.935064\n",
      "Grosse_Erweckung#3 0.934708\n",
      "armer_Hund#1 0.934650\n",
      "Alois_Windisch#2 0.934103\n",
      "eingezogener_spitzbogiger#1 0.933642\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"Hund\"\n",
    "for sense_id, prob in sv.get_senses(word):\n",
    "    print(sense_id)\n",
    "    print(\"=\"*20)\n",
    "    for rsense_id, sim in sv.wv.most_similar(sense_id):\n",
    "        print(\"{} {:f}\".format(rsense_id, sim))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word sense disambiguation: loading word embeddings\n",
    "\n",
    "To use our word sense disambiguation mechanism you also need word vectors or context vectors, depending on the dismabiguation  strategy. Those word are located in the ``model`` directory and has the extension ``.vectors``.\n",
    "\n",
    "Our WSD mechanism is based on word similarities (`sim`) and requires word vectors to represent context words. In following we provide a disambiguation example using similarity strategy.\n",
    "\n",
    "First, load word vectors using gensim library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors_fpath = \"model/dewiki.txt.word_vectors\"\n",
    "wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialise the WSD object with sense and word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disambiguation method: sim\n",
      "Filter context: f = 3\n"
     ]
    }
   ],
   "source": [
    "from wsd import WSD\n",
    "wsd_model = WSD(sv, wv, window=5, method='sim', filter_ctx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings have the following meaning: it will extract at most `window`*2 words around the target word from the  sentence as context and it will use only three most discriminative context words for disambiguation.\n",
    "\n",
    "Now you can disambiguate the word \"table\" in the sentence \"They bought a table and chairs for kitchen\" using       `dis_text` function. As input it takes a sentence with space separated tokens, a target word, and start/end indices of the target word in the given sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hund#1', [0.11995293847479865, -0.10629601676025206])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Hund\"\n",
    "context = \"Die beste Voraussetzung für die Hund-Katze-Freundschaft ist, dass keiner von beiden in der Vergangenheit unangenehme Erlebnisse mit der anderen Gattung hatte. Am einfachsten ist die ungleiche WG, wenn sich zwei Jungtiere ein Zuhause teilen. Bei erwachsenen Tieren ist es einfacher, wenn sich Miezi in Bellos Haushalt einnistet – nicht umgekehrt, da Hunde Rudeltiere sind. Damit ein Hund das Kätzchen aber auch als Rudelmitglied sieht und nicht als Futter sollten ein paar Regeln beachtet werden\"\n",
    "wsd_model.dis_text(context, word, 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the four steps described above together\n",
    "\n",
    "An example of words sense induction, in this case for the English language \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sensegram\n",
    "from wsd import WSD\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Input data and paths (see README for model download information)\n",
    "sense_vectors_fpath = \"model/wiki.txt.clusters.minsize5-1000-sum-score-20.sense_vectors\"\n",
    "word_vectors_fpath = \"model/wiki.txt.word_vectors\"\n",
    "context_words_max = 3 # change this paramters to 1, 2, 5, 10, 15, 20 : it may improve the results\n",
    "context_window_size = 5 # this parameters can be also changed during experiments \n",
    "word = \"python\"\n",
    "context = \"Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991, Python has a design philosophy that emphasizes code readability, notably using significant whitespace.\"\n",
    "ignore_case = True\n",
    "lang = \"en\" # to filter out stopwords\n",
    "\n",
    "# Load models (takes long time)\n",
    "sv = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)\n",
    "wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "# Play with the model (is quick)\n",
    "print(\"Probabilities of the senses:\\n{}\\n\\n\".format(sv.get_senses(word, ignore_case=ignore_case)))\n",
    "\n",
    "for sense_id, prob in sv.get_senses(word, ignore_case=ignore_case):\n",
    "    print(sense_id)\n",
    "    print(\"=\"*20)\n",
    "    for rsense_id, sim in sv.wv.most_similar(sense_id):\n",
    "        print(\"{} {:f}\".format(rsense_id, sim))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Disambiguate a word in a context\n",
    "wsd_model = WSD(sv, wv, window=context_window_size, lang=lang,\n",
    "                filter_ctx=context_words_max, ignore_case=ignore_case)    \n",
    "print(wsd_model.disambiguate(context, word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDEWaC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of the senses:\n",
      "[('Maus#1', 0.885167), ('Maus#2', 0.114833), ('maus#1', 1.0)]\n",
      "\n",
      "\n",
      "Maus#1\n",
      "====================\n",
      "Wanze#1 0.941433\n",
      "Mausefalle#1 0.911212\n",
      "Libelle#1 0.908983\n",
      "Puppe#2 0.908126\n",
      "Kakerlake#1 0.907264\n",
      "Ratte#1 0.904579\n",
      "Spinne#1 0.904541\n",
      "Garnele#1 0.904265\n",
      "Biene#1 0.902711\n",
      "Schnecke#1 0.902516\n",
      "\n",
      "\n",
      "Maus#2\n",
      "====================\n",
      "Cursortasten#1 0.972510\n",
      "Pfeiltasten#1 0.964843\n",
      "Richtungstasten#1 0.959181\n",
      "Cursor_Tasten#1 0.955643\n",
      "linken_Maustaste#1 0.945940\n",
      "Tab_Taste#1 0.944255\n",
      "gedrückter_Maustaste#2 0.937142\n",
      "rechten_Maustaste#1 0.935822\n",
      "Mausbewegung#1 0.935422\n",
      "Sprungtaste#2 0.933055\n",
      "\n",
      "\n",
      "maus#1\n",
      "====================\n",
      "hp#2 0.970098\n",
      "gedichte#1 0.954409\n",
      "galerie#1 0.951155\n",
      "meinem_bild#3 0.945498\n",
      "fotografie#1 0.941011\n",
      "alex#1 0.939576\n",
      "wolf#1 0.935995\n",
      "ds#1 0.934641\n",
      "nachricht#3 0.934026\n",
      "anfrage#1 0.933526\n",
      "\n",
      "\n",
      "Disambiguation method: sim\n",
      "Filter context: f = 3\n",
      "Context words:\n",
      "Eingabegerät\t0.394\n",
      "('Maus#2', [0.34084959179662255, 0.52808930616226313, 0.13438711940973358])\n"
     ]
    }
   ],
   "source": [
    "import sensegram\n",
    "from wsd import WSD\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Input data and paths\n",
    "sense_vectors_fpath = \"model/sdewac-v3.corpus.clusters.minsize5-1000-sum-score-20.sense_vectors\"\n",
    "word_vectors_fpath = \"model/sdewac-v3.corpus.word_vectors\"\n",
    "context_words_max = 3 # change this paramters to 1, 2, 5, 10, 15, 20 : it may improve the results\n",
    "context_window_size = 5 # this parameters can be also changed during experiments \n",
    "word = \"Maus\"\n",
    "context = \"Die Maus ist ein Eingabegerät (Befehlsgeber) bei Computern. Der allererste Prototyp wurde 1963 nach Zeichnungen von Douglas C. Engelbart gebaut; seit Mitte der 1980er Jahre bildet die Maus für fast alle Computertätigkeiten zusammen mit dem Monitor und der Tastatur eine der wichtigsten Mensch-Maschine-Schnittstellen. Die Entwicklung grafischer Benutzeroberflächen hat die Computermaus zu einem heute praktisch an jedem Desktop-PC verfügbaren Standardeingabegerät gemacht.\"\n",
    "ignore_case = True\n",
    "\n",
    "# Load models (takes long time)\n",
    "sv = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)\n",
    "wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "# Play with the model (is quick)\n",
    "print(\"Probabilities of the senses:\\n{}\\n\\n\".format(sv.get_senses(word, ignore_case=ignore_case)))\n",
    "\n",
    "for sense_id, prob in sv.get_senses(word, ignore_case=ignore_case):\n",
    "    print(sense_id)\n",
    "    print(\"=\"*20)\n",
    "    for rsense_id, sim in sv.wv.most_similar(sense_id):\n",
    "        print(\"{} {:f}\".format(rsense_id, sim))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Disambiguate a word in a context\n",
    "wsd_model = WSD(sv, wv, window=context_window_size, lang=\"de\",\n",
    "                filter_ctx=context_words_max, ignore_case=ignore_case)    \n",
    "print(wsd_model.disambiguate(context, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense induction exepriment for the Russian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ../russe-wsi-kit/data/main/wiki-wiki/test.csv.pred.csv\n",
      "Output: ../russe-wsi-kit/data/main/bts-rnc/test.csv.pred.csv\n",
      "Output: ../russe-wsi-kit/data/main/active-dict/test.csv.pred.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../russe-wsi-kit/data/main/active-dict/test.csv.pred.csv'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sensegram\n",
    "from wsd import WSD\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Input data and paths\n",
    "sense_vectors_fpath = \"model/wikipedia-ru-2018.txt.clusters.minsize5-1000-sum-score-20.sense_vectors\"\n",
    "word_vectors_fpath = \"model/wikipedia-ru-2018.txt.word_vectors\"\n",
    "max_context_words = 5 # change this paramters to 1, 2, 5, 10, 15, 20 : it may improve the results\n",
    "context_window_size = 20 # this parameters can be also changed during experiments \n",
    "word = \"ключ\"\n",
    "context = \"Ключ — это секретная информация, используемая криптографическим алгоритмом при зашифровании/расшифровании сообщений, постановке и проверке цифровой подписи, вычислении кодов аутентичности (MAC). При использовании одного и того же алгоритма результат шифрования зависит от ключа. Для современных алгоритмов сильной криптографии утрата ключа приводит к практической невозможности расшифровать информацию.\"\n",
    "ignore_case = False\n",
    "lang = \"ru\" # to filter out stopwords\n",
    "\n",
    "# Load models (takes long time)\n",
    "# sv = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)\n",
    "# wv = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "# Play with the model (is quick)\n",
    "# print(\"Probabilities of the senses:\\n{}\\n\\n\".format(sv.get_senses(word, ignore_case=ignore_case)))\n",
    "\n",
    "# for sense_id, prob in sv.get_senses(word, ignore_case=ignore_case):\n",
    "#     print(sense_id)\n",
    "#     print(\"=\"*20)\n",
    "#     for rsense_id, sim in sv.wv.most_similar(sense_id):\n",
    "#         print(\"{} {:f}\".format(rsense_id, sim))\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# Disambiguate a word in a context\n",
    "wsd_model = WSD(sv, wv, window=context_window_size, lang=lang,\n",
    "                max_context_words=max_context_words, ignore_case=ignore_case)    \n",
    "# print(wsd_model.disambiguate(context, word))\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "from pandas import read_csv \n",
    "\n",
    "# you can download the WSI evaluation dataset with 'git clone https://github.com/nlpub/russe-wsi-kit.git'\n",
    "wikiwiki_fpath = \"../russe-wsi-kit/data/main/wiki-wiki/test.csv\"\n",
    "activedict_fpath = \"../russe-wsi-kit/data/main/active-dict/test.csv\"\n",
    "btsrnc_fpath = \"../russe-wsi-kit/data/main/bts-rnc/test.csv\"\n",
    "\n",
    "def evaluate(dataset_fpath):\n",
    "    output_fpath = dataset_fpath + \".pred.csv\"\n",
    "\n",
    "    df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sense_id, _ = wsd_model.disambiguate(row.context, row.word)\n",
    "        df.loc[i, \"predict_sense_id\"] = sense_id\n",
    "\n",
    "    df.to_csv(output_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "    print(\"Output:\", output_fpath)\n",
    "    \n",
    "    return output_fpath\n",
    "\n",
    "evaluate(wikiwiki_fpath)\n",
    "evaluate(btsrnc_fpath)\n",
    "evaluate(activedict_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
